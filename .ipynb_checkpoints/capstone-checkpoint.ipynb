{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH= \"/Users/srikeshnagoji/Documents/PythonWorkSpace/jupyter_lab_workspace/PES/CAPSTONE/kaggle_3m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/Users/srikeshnagoji/Documents/PythonWorkSpace/jupyter_lab_workspace/PES/CAPSTONE/kaggle_3m/TCGA_CS_4941_19960909/TCGA_CS_4941_19960909_1.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LEN = len(BASE_PATH) + len(\"/TCGA_CS_4941_19960909/TCGA_CS_4941_19960909_\")\n",
    "END_LEN = len(\".tif\")\n",
    "END_MASK_LEN = len(\"_mask.tif\")\n",
    "\n",
    "IMG_SIZE = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for dir_ in os.listdir(BASE_PATH):\n",
    "    dir_path = os.path.join(BASE_PATH, dir_)\n",
    "    if os.path.isdir(dir_path):\n",
    "        for filename in os.listdir(dir_path):\n",
    "            img_path = os.path.join(dir_path, filename)\n",
    "            data.append([dir_, img_path])\n",
    "    else:\n",
    "        print(f\"[INFO] This is not a dir --> {dir_path}\")\n",
    "        \n",
    "df = pd.DataFrame(data, columns=[\"dir_name\", \"image_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs = df[~df[\"image_path\"].str.contains(\"mask\")]\n",
    "df_masks = df[df[\"image_path\"].str.contains(\"mask\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs.iloc[0,1][BASE_LEN: -END_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = sorted(df_imgs[\"image_path\"].values, key= lambda x: int((x[BASE_LEN: -END_LEN])))\n",
    "masks = sorted(df_masks[\"image_path\"].values, key=lambda x: int((x[BASE_LEN: -END_MASK_LEN])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "idx = random.randint(0, len(imgs)-1)\n",
    "print(f\"This image *{imgs[idx]}*\\n Belongs to the mask *{masks[idx]}*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataframe\n",
    "dff = pd.DataFrame({\"patient\": df_imgs.dir_name.values,\n",
    "                   \"image_path\": imgs,\n",
    "                   \"mask_path\": masks})\n",
    "\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.iloc[0,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_neg_diagnosis(mask_path):\n",
    "    val = np.max(cv2.imread(mask_path))\n",
    "    if val > 0: return 1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff[\"diagnosis\"] = dff[\"mask_path\"].apply(lambda x: pos_neg_diagnosis(x))\n",
    "\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amount of patients: \", len(set(dff.patient)))\n",
    "print(\"Amount of records: \", len(dff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import sklearn as sk\n",
    "import sys\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch,'has_mps',False)\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "print(f\"Target device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainMRIDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(self.df.iloc[idx, 1])\n",
    "        mask = cv2.imread(self.df.iloc[idx, 2], 0)\n",
    "        \n",
    "        augmented = self.transform(image=image,\n",
    "                                   mask=mask)\n",
    "        \n",
    "        image = augmented[\"image\"]\n",
    "        mask = augmented[\"mask\"]\n",
    "#         mask = np.expand_dims(augmented[\"mask\"], axis=0)\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 128\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(width = PATCH_SIZE, height = PATCH_SIZE, p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),\n",
    "\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensor(),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(dff, stratify=dff.diagnosis, test_size=0.1)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "train_df, test_df = train_test_split(train_df, stratify=train_df.diagnosis, test_size=0.12)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {train_df.shape} \\nVal: {val_df.shape} \\nTest: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BrainMRIDataset(train_df, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=26, shuffle=True)\n",
    "\n",
    "val_dataset = BrainMRIDataset(val_df, transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=26,  shuffle=True)\n",
    "\n",
    "test_dataset = BrainMRIDataset(test_df, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=26,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_aug(inputs, nrows=5, ncols=5, norm=False):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplots_adjust(wspace=0., hspace=0.)\n",
    "    i_ = 0\n",
    "    \n",
    "    if len(inputs) > 25:\n",
    "        inputs = inputs[:25]\n",
    "        \n",
    "    for idx in range(len(inputs)):\n",
    "    \n",
    "        # normalization\n",
    "        if norm:           \n",
    "            img = inputs[idx].numpy().transpose(1,2,0)\n",
    "            mean = [0.485, 0.456, 0.406]\n",
    "            std = [0.229, 0.224, 0.225] \n",
    "            img = (img*std+mean).astype(np.float32)\n",
    "            \n",
    "        else:\n",
    "            img = inputs[idx].numpy().astype(np.float32)\n",
    "            img = img[0,:,:]\n",
    "        \n",
    "        plt.subplot(nrows, ncols, i_+1)\n",
    "        plt.imshow(img); \n",
    "        plt.axis('off')\n",
    " \n",
    "        i_ += 1\n",
    "        \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks = next(iter(train_dataloader))\n",
    "print(images.shape, masks.shape)\n",
    "\n",
    "show_aug(images)\n",
    "show_aug(masks, norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                                  nn.Conv2d(ch_in, ch_out,\n",
    "                                            kernel_size=3, stride=1,\n",
    "                                            padding=1, bias=True),\n",
    "                                  nn.BatchNorm2d(ch_out),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  nn.Conv2d(ch_out, ch_out,\n",
    "                                            kernel_size=3, stride=1,\n",
    "                                            padding=1, bias=True),\n",
    "                                  nn.BatchNorm2d(ch_out),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpConvBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "                                nn.Upsample(scale_factor=2),\n",
    "                                nn.Conv2d(ch_in, ch_out,\n",
    "                                         kernel_size=3,stride=1,\n",
    "                                         padding=1, bias=True),\n",
    "                                nn.BatchNorm2d(ch_out),\n",
    "                                nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x = self.up(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, f_g, f_l, f_int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w_g = nn.Sequential(\n",
    "                                nn.Conv2d(f_g, f_int,\n",
    "                                         kernel_size=1, stride=1,\n",
    "                                         padding=0, bias=True),\n",
    "                                nn.BatchNorm2d(f_int)\n",
    "        )\n",
    "        \n",
    "        self.w_x = nn.Sequential(\n",
    "                                nn.Conv2d(f_l, f_int,\n",
    "                                         kernel_size=1, stride=1,\n",
    "                                         padding=0, bias=True),\n",
    "                                nn.BatchNorm2d(f_int)\n",
    "        )\n",
    "        \n",
    "        self.psi = nn.Sequential(\n",
    "                                nn.Conv2d(f_int, 1,\n",
    "                                         kernel_size=1, stride=1,\n",
    "                                         padding=0,  bias=True),\n",
    "                                nn.BatchNorm2d(1),\n",
    "                                nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, g, x):\n",
    "        g1 = self.w_g(g)\n",
    "        x1 = self.w_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "        \n",
    "        return psi*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sanity\n",
    "output = torch.randn(1,3,256,256).to(device)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNET 3+ with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def weights_init_xavier(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.xavier_normal_(m.weight.data, gain=1)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.xavier_normal_(m.weight.data, gain=1)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def weights_init_kaiming(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def weights_init_orthogonal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.orthogonal_(m.weight.data, gain=1)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.orthogonal_(m.weight.data, gain=1)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def init_weights(net, init_type='normal'):\n",
    "    #print('initialization method [%s]' % init_type)\n",
    "    if init_type == 'normal':\n",
    "        net.apply(weights_init_normal)\n",
    "    elif init_type == 'xavier':\n",
    "        net.apply(weights_init_xavier)\n",
    "    elif init_type == 'kaiming':\n",
    "        net.apply(weights_init_kaiming)\n",
    "    elif init_type == 'orthogonal':\n",
    "        net.apply(weights_init_orthogonal)\n",
    "    else:\n",
    "        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unetConv2(nn.Module):\n",
    "    def __init__(self, in_size, out_size, is_batchnorm, n=2, ks=3, stride=1, padding=1):\n",
    "        super(unetConv2, self).__init__()\n",
    "        self.n = n\n",
    "        self.ks = ks\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        s = stride\n",
    "        p = padding\n",
    "        if is_batchnorm:\n",
    "            for i in range(1, n + 1):\n",
    "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
    "                                     nn.BatchNorm2d(out_size),\n",
    "                                     nn.ReLU(inplace=True), )\n",
    "                setattr(self, 'conv%d' % i, conv)\n",
    "                in_size = out_size\n",
    "\n",
    "        else:\n",
    "            for i in range(1, n + 1):\n",
    "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
    "                                     nn.ReLU(inplace=True), )\n",
    "                setattr(self, 'conv%d' % i, conv)\n",
    "                in_size = out_size\n",
    "\n",
    "        # initialise the blocks\n",
    "        for m in self.children():\n",
    "            init_weights(m, init_type='kaiming')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        for i in range(1, self.n + 1):\n",
    "            conv = getattr(self, 'conv%d' % i)\n",
    "            x = conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST Unet 3+\n",
    "\n",
    "class UNet3Plus(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1, bilinear=True, feature_scale=4,\n",
    "                 is_deconv=True, is_batchnorm=True):\n",
    "        super(UNet3Plus, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        self.feature_scale = feature_scale\n",
    "        self.is_deconv = is_deconv\n",
    "        self.is_batchnorm = is_batchnorm\n",
    "        filters = [64, 128, 256, 512, 1024]\n",
    "\n",
    "        ## -------------Encoder--------------\n",
    "        self.conv1 = unetConv2(self.n_channels, filters[0], self.is_batchnorm)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = unetConv2(filters[0], filters[1], self.is_batchnorm)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv3 = unetConv2(filters[1], filters[2], self.is_batchnorm)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv4 = unetConv2(filters[2], filters[3], self.is_batchnorm)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv5 = unetConv2(filters[3], filters[4], self.is_batchnorm)\n",
    "\n",
    "        ## -------------Decoder--------------\n",
    "        self.CatChannels = filters[0]\n",
    "        self.CatBlocks = 5\n",
    "        self.UpChannels = self.CatChannels * self.CatBlocks\n",
    "\n",
    "        '''stage 4d'''\n",
    "        # h1->320*320, hd4->40*40, Pooling 8 times\n",
    "        self.h1_PT_hd4 = nn.MaxPool2d(8, 8, ceil_mode=True)\n",
    "        self.h1_PT_hd4_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_PT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h1_PT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h2->160*160, hd4->40*40, Pooling 4 times\n",
    "        self.h2_PT_hd4 = nn.MaxPool2d(4, 4, ceil_mode=True)\n",
    "        self.h2_PT_hd4_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
    "        self.h2_PT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h2_PT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h3->80*80, hd4->40*40, Pooling 2 times\n",
    "        self.h3_PT_hd4 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.h3_PT_hd4_conv = nn.Conv2d(filters[2], self.CatChannels, 3, padding=1)\n",
    "        self.h3_PT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h3_PT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h4->40*40, hd4->40*40, Concatenation\n",
    "        self.h4_Cat_hd4_conv = nn.Conv2d(filters[3], self.CatChannels, 3, padding=1)\n",
    "        self.h4_Cat_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h4_Cat_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd4->40*40, Upsample 2 times\n",
    "        self.hd5_UT_hd4 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd4_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_PT_hd4, h2_PT_hd4, h3_PT_hd4, h4_Cat_hd4, hd5_UT_hd4)\n",
    "        self.conv4d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "        self.bn4d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "        self.relu4d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        '''stage 3d'''\n",
    "        # h1->320*320, hd3->80*80, Pooling 4 times\n",
    "        self.h1_PT_hd3 = nn.MaxPool2d(4, 4, ceil_mode=True)\n",
    "        self.h1_PT_hd3_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_PT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h1_PT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h2->160*160, hd3->80*80, Pooling 2 times\n",
    "        self.h2_PT_hd3 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.h2_PT_hd3_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
    "        self.h2_PT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h2_PT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h3->80*80, hd3->80*80, Concatenation\n",
    "        self.h3_Cat_hd3_conv = nn.Conv2d(filters[2], self.CatChannels, 3, padding=1)\n",
    "        self.h3_Cat_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h3_Cat_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd4->40*40, hd4->80*80, Upsample 2 times\n",
    "        self.hd4_UT_hd3 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd4_UT_hd3_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd4_UT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd4_UT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd4->80*80, Upsample 4 times\n",
    "        self.hd5_UT_hd3 = nn.Upsample(scale_factor=4, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd3_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd4_UT_hd3, hd5_UT_hd3)\n",
    "        self.conv3d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "        self.bn3d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "        self.relu3d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        '''stage 2d '''\n",
    "        # h1->320*320, hd2->160*160, Pooling 2 times\n",
    "        self.h1_PT_hd2 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.h1_PT_hd2_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_PT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h1_PT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h2->160*160, hd2->160*160, Concatenation\n",
    "        self.h2_Cat_hd2_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
    "        self.h2_Cat_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h2_Cat_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd3->80*80, hd2->160*160, Upsample 2 times\n",
    "        self.hd3_UT_hd2 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd3_UT_hd2_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd3_UT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd3_UT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd4->40*40, hd2->160*160, Upsample 4 times\n",
    "        self.hd4_UT_hd2 = nn.Upsample(scale_factor=4, mode='bilinear')  # 14*14\n",
    "        self.hd4_UT_hd2_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd4_UT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd4_UT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd2->160*160, Upsample 8 times\n",
    "        self.hd5_UT_hd2 = nn.Upsample(scale_factor=8, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd2_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_PT_hd2, h2_Cat_hd2, hd3_UT_hd2, hd4_UT_hd2, hd5_UT_hd2)\n",
    "        self.conv2d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "        self.bn2d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "        self.relu2d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        '''stage 1d'''\n",
    "        # h1->320*320, hd1->320*320, Concatenation\n",
    "        self.h1_Cat_hd1_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_Cat_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h1_Cat_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd2->160*160, hd1->320*320, Upsample 2 times\n",
    "        self.hd2_UT_hd1 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd2_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd2_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd2_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd3->80*80, hd1->320*320, Upsample 4 times\n",
    "        self.hd3_UT_hd1 = nn.Upsample(scale_factor=4, mode='bilinear')  # 14*14\n",
    "        self.hd3_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd3_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd3_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd4->40*40, hd1->320*320, Upsample 8 times\n",
    "        self.hd4_UT_hd1 = nn.Upsample(scale_factor=8, mode='bilinear')  # 14*14\n",
    "        self.hd4_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd4_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd4_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd1->320*320, Upsample 16 times\n",
    "        self.hd5_UT_hd1 = nn.Upsample(scale_factor=16, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd1_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_Cat_hd1, hd2_UT_hd1, hd3_UT_hd1, hd4_UT_hd1, hd5_UT_hd1)\n",
    "        self.conv1d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "        self.bn1d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "        self.relu1d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # output\n",
    "        self.outconv1 = nn.Conv2d(self.UpChannels, n_classes, 3, padding=1)\n",
    "\n",
    "        # initialise weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init_weights(m, init_type='kaiming')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init_weights(m, init_type='kaiming')\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## -------------Encoder-------------\n",
    "        h1 = self.conv1(inputs)  # h1->320*320*64\n",
    "\n",
    "        h2 = self.maxpool1(h1)\n",
    "        h2 = self.conv2(h2)  # h2->160*160*128\n",
    "\n",
    "        h3 = self.maxpool2(h2)\n",
    "        h3 = self.conv3(h3)  # h3->80*80*256\n",
    "\n",
    "        h4 = self.maxpool3(h3)\n",
    "        h4 = self.conv4(h4)  # h4->40*40*512\n",
    "\n",
    "        h5 = self.maxpool4(h4)\n",
    "        hd5 = self.conv5(h5)  # h5->20*20*1024\n",
    "\n",
    "        ## -------------Decoder-------------\n",
    "        h1_PT_hd4 = self.h1_PT_hd4_relu(self.h1_PT_hd4_bn(self.h1_PT_hd4_conv(self.h1_PT_hd4(h1))))\n",
    "        h2_PT_hd4 = self.h2_PT_hd4_relu(self.h2_PT_hd4_bn(self.h2_PT_hd4_conv(self.h2_PT_hd4(h2))))\n",
    "        h3_PT_hd4 = self.h3_PT_hd4_relu(self.h3_PT_hd4_bn(self.h3_PT_hd4_conv(self.h3_PT_hd4(h3))))\n",
    "        h4_Cat_hd4 = self.h4_Cat_hd4_relu(self.h4_Cat_hd4_bn(self.h4_Cat_hd4_conv(h4)))\n",
    "        hd5_UT_hd4 = self.hd5_UT_hd4_relu(self.hd5_UT_hd4_bn(self.hd5_UT_hd4_conv(self.hd5_UT_hd4(hd5))))\n",
    "        hd4 = self.relu4d_1(self.bn4d_1(self.conv4d_1(torch.cat((h1_PT_hd4, h2_PT_hd4, h3_PT_hd4, h4_Cat_hd4, hd5_UT_hd4), 1)))) # hd4->40*40*UpChannels\n",
    "\n",
    "        h1_PT_hd3 = self.h1_PT_hd3_relu(self.h1_PT_hd3_bn(self.h1_PT_hd3_conv(self.h1_PT_hd3(h1))))\n",
    "        h2_PT_hd3 = self.h2_PT_hd3_relu(self.h2_PT_hd3_bn(self.h2_PT_hd3_conv(self.h2_PT_hd3(h2))))\n",
    "        h3_Cat_hd3 = self.h3_Cat_hd3_relu(self.h3_Cat_hd3_bn(self.h3_Cat_hd3_conv(h3)))\n",
    "        hd4_UT_hd3 = self.hd4_UT_hd3_relu(self.hd4_UT_hd3_bn(self.hd4_UT_hd3_conv(self.hd4_UT_hd3(hd4))))\n",
    "        hd5_UT_hd3 = self.hd5_UT_hd3_relu(self.hd5_UT_hd3_bn(self.hd5_UT_hd3_conv(self.hd5_UT_hd3(hd5))))\n",
    "        hd3 = self.relu3d_1(self.bn3d_1(self.conv3d_1(torch.cat((h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd4_UT_hd3, hd5_UT_hd3), 1)))) # hd3->80*80*UpChannels\n",
    "\n",
    "        h1_PT_hd2 = self.h1_PT_hd2_relu(self.h1_PT_hd2_bn(self.h1_PT_hd2_conv(self.h1_PT_hd2(h1))))\n",
    "        h2_Cat_hd2 = self.h2_Cat_hd2_relu(self.h2_Cat_hd2_bn(self.h2_Cat_hd2_conv(h2)))\n",
    "        hd3_UT_hd2 = self.hd3_UT_hd2_relu(self.hd3_UT_hd2_bn(self.hd3_UT_hd2_conv(self.hd3_UT_hd2(hd3))))\n",
    "        hd4_UT_hd2 = self.hd4_UT_hd2_relu(self.hd4_UT_hd2_bn(self.hd4_UT_hd2_conv(self.hd4_UT_hd2(hd4))))\n",
    "        hd5_UT_hd2 = self.hd5_UT_hd2_relu(self.hd5_UT_hd2_bn(self.hd5_UT_hd2_conv(self.hd5_UT_hd2(hd5))))\n",
    "        hd2 = self.relu2d_1(self.bn2d_1(self.conv2d_1(torch.cat((h1_PT_hd2, h2_Cat_hd2, hd3_UT_hd2, hd4_UT_hd2, hd5_UT_hd2), 1)))) # hd2->160*160*UpChannels\n",
    "\n",
    "        h1_Cat_hd1 = self.h1_Cat_hd1_relu(self.h1_Cat_hd1_bn(self.h1_Cat_hd1_conv(h1)))\n",
    "        hd2_UT_hd1 = self.hd2_UT_hd1_relu(self.hd2_UT_hd1_bn(self.hd2_UT_hd1_conv(self.hd2_UT_hd1(hd2))))\n",
    "        hd3_UT_hd1 = self.hd3_UT_hd1_relu(self.hd3_UT_hd1_bn(self.hd3_UT_hd1_conv(self.hd3_UT_hd1(hd3))))\n",
    "        hd4_UT_hd1 = self.hd4_UT_hd1_relu(self.hd4_UT_hd1_bn(self.hd4_UT_hd1_conv(self.hd4_UT_hd1(hd4))))\n",
    "        hd5_UT_hd1 = self.hd5_UT_hd1_relu(self.hd5_UT_hd1_bn(self.hd5_UT_hd1_conv(self.hd5_UT_hd1(hd5))))\n",
    "        hd1 = self.relu1d_1(self.bn1d_1(self.conv1d_1(torch.cat((h1_Cat_hd1, hd2_UT_hd1, hd3_UT_hd1, hd4_UT_hd1, hd5_UT_hd1), 1)))) # hd1->320*320*UpChannels\n",
    "\n",
    "        d1 = self.outconv1(hd1)  # d1->320*320*n_classes\n",
    "        return F.sigmoid(d1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet3p = UNet3Plus(n_classes=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEGMENTATION METRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef_metric(inputs, target):\n",
    "    intersection = 2.0 * (target*inputs).sum()\n",
    "    union = target.sum() + inputs.sum()\n",
    "    if target.sum() == 0 and inputs.sum() == 0:\n",
    "        return 1.0 \n",
    "    return intersection/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segmentation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "#         inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "DiceLoss()(torch.tensor([0.7, 1., 1.]), \n",
    "              torch.tensor([1.,1.,1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model_unet_3p.pt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, model, train_loader, val_loader, train_loss, optimizer, lr_scheduler, num_epochs):\n",
    "    print(f\"[INFO] Model is initializing... {model_name}\")\n",
    "    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        losses = []\n",
    "        train_iou = []\n",
    "        \n",
    "        for i_step, (data, target) in enumerate(tqdm(train_loader)):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            \n",
    "            out_cut = np.copy(outputs.data.cpu().numpy())\n",
    "            out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n",
    "            out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n",
    "            \n",
    "            train_dice = dice_coef_metric(out_cut, target.data.cpu().numpy())\n",
    "            \n",
    "            loss = train_loss(outputs, target)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            train_iou.append(train_dice)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        val_mean_iou = compute_iou(model, val_loader)\n",
    "        \n",
    "        loss_history.append(np.array(losses).mean())\n",
    "        train_history.append(np.array(train_iou).mean())\n",
    "        val_history.append(val_mean_iou)\n",
    "        \n",
    "        \n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': np.array(losses).mean(),\n",
    "                    }, PATH)\n",
    "        \n",
    "        print(\"Epoch [%d]\" % (epoch))\n",
    "        print(\"Mean loss on train:\", np.array(losses).mean(), \n",
    "              \"\\nMean DICE on train:\", np.array(train_iou).mean(), \n",
    "              \"\\nMean DICE on validation:\", val_mean_iou)\n",
    "        \n",
    "    return loss_history, train_history, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(model, loader, threshold=0.3):\n",
    "    valloss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i_step, (data, target) in enumerate(loader):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "\n",
    "            out_cut = np.copy(outputs.data.cpu().numpy())\n",
    "            out_cut[np.nonzero(out_cut < threshold)] = 0.0\n",
    "            out_cut[np.nonzero(out_cut >= threshold)] = 1.0\n",
    "            picloss = dice_coef_metric(out_cut, target.data.cpu().numpy())\n",
    "            valloss += picloss\n",
    "\n",
    "    return valloss / i_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adamax(unet3p.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "num_ep = 20\n",
    "# after 30 does not improve much\n",
    "\n",
    "aun_lh, aun_th, aun_vh = train_model(\"UNet 3p\", unet3p, train_dataloader, val_dataloader, DiceLoss(), opt, False, num_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_name,\n",
    "                        train_history, val_history, \n",
    "                        num_epochs):\n",
    "    \n",
    "    x = np.arange(num_epochs)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, train_history, label='train dice', lw=3, c=\"springgreen\")\n",
    "    plt.plot(x, val_history, label='validation dice', lw=3, c=\"deeppink\")\n",
    "\n",
    "    plt.title(f\"{model_name}\", fontsize=15)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel(\"Epoch\", fontsize=15)\n",
    "    plt.ylabel(\"DICE\", fontsize=15)\n",
    "\n",
    "    fn = str(int(time.time())) + \".png\"\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(\"U-Net 3+\", aun_th, aun_vh, num_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_ep), aun_lh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iou = compute_iou(unet3p, test_dataloader)\n",
    "print(f\"\"\" U-Net 3P\\nMean IoU of the test images - {np.around(test_iou, 2)*100}%\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Loss Functions and Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: copy paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d9fe2f040c093e6866475fa96b829ef49234fde465e00105dee1d04d2ea6c922"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
